{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5-rC-4moM3S"
   },
   "source": [
    "# Boosting a decision stump\n",
    "\n",
    "The goal of this notebook is to implement your own boosting module.\n",
    "\n",
    "* Go through an implementation of decision trees.\n",
    "* Implement Adaboost ensembling.\n",
    "* Use your implementation of Adaboost to train a boosted decision stump ensemble.\n",
    "* Evaluate the effect of boosting (adding more decision stumps) on performance of the model.\n",
    "* Explore the robustness of Adaboost to overfitting.\n",
    "\n",
    "*This file is adapted from course material by Carlos Guestrin and Emily Fox.*\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6DxWxi0oM3V"
   },
   "source": [
    "## Import some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gaWW7kIGoM3V"
   },
   "outputs": [],
   "source": [
    "## please make sure that the packages are updated to the newest version. \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQq7bnHRoM3W"
   },
   "source": [
    "# Getting the data ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1sUFKWeqoM3X"
   },
   "source": [
    "Load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "SoBeEgzooM3X"
   },
   "outputs": [],
   "source": [
    "loans = pd.read_csv('loan_small.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nF-dZJLFoM3X"
   },
   "source": [
    "### Recoding the target column\n",
    "\n",
    "We re-assign the target to have +1 as a safe (good) loan, and -1 as a risky (bad) loan. In the next cell, the features are also briefly explained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3Gn9x6cKoM3Y"
   },
   "outputs": [],
   "source": [
    "features = ['grade',              # grade of the loan\n",
    "            'term',               # the term of the loan\n",
    "            'home_ownership',     # home ownership status: own, mortgage or rent\n",
    "            'emp_length',         # number of years of employment\n",
    "           ]\n",
    "\n",
    "loans['safe_loans'] = loans['loan_status'].apply(lambda x : +1 if x=='Fully Paid' else -1)\n",
    "\n",
    "## please update pandas to the newest version in order to execute the following line\n",
    "loans.drop(columns=['loan_status'], inplace=True)\n",
    "\n",
    "target = 'safe_loans' # this variable will be used later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCCDPp-poM3Y"
   },
   "source": [
    "### Transform categorical data into binary features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1AWgBY2coM3Z"
   },
   "source": [
    "In this assignment, we will work with **binary decision trees**. Since all of our features are currently categorical features, we want to turn them into binary features using 1-hot encoding. \n",
    "\n",
    "We can do so with the following code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8M7uCoiToM3Z"
   },
   "outputs": [],
   "source": [
    "loans = pd.get_dummies(loans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mcdp9_lxoM3Z"
   },
   "source": [
    "Let's see what the feature columns look like now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "WXGDufmRoM3a",
    "outputId": "e058de86-2a8a-49ca-e883-944a0d5eeaed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['term_ 36 months',\n",
       " 'term_ 60 months',\n",
       " 'grade_A',\n",
       " 'grade_B',\n",
       " 'grade_C',\n",
       " 'grade_D',\n",
       " 'grade_E',\n",
       " 'grade_F',\n",
       " 'grade_G',\n",
       " 'home_ownership_MORTGAGE',\n",
       " 'home_ownership_NONE',\n",
       " 'home_ownership_OTHER',\n",
       " 'home_ownership_OWN',\n",
       " 'home_ownership_RENT',\n",
       " 'emp_length_1 year',\n",
       " 'emp_length_10+ years',\n",
       " 'emp_length_2 years',\n",
       " 'emp_length_3 years',\n",
       " 'emp_length_4 years',\n",
       " 'emp_length_5 years',\n",
       " 'emp_length_6 years',\n",
       " 'emp_length_7 years',\n",
       " 'emp_length_8 years',\n",
       " 'emp_length_9 years',\n",
       " 'emp_length_< 1 year']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = list(loans.columns)\n",
    "features.remove('safe_loans')  # Remove the response variable\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wAraLisoM3a"
   },
   "source": [
    "### Train-test split\n",
    "\n",
    "We split the data into training and test sets with 80% of the data in the training set and 20% of the data in the test set. We use `seed=1` so that everyone gets the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jiJ0_K3poM3a"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(loans, test_size = 0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Iw8k0pyoM3b"
   },
   "source": [
    "# Weighted decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8qDfbH-FoM3b"
   },
   "source": [
    "Since the data weights change as we build an AdaBoost model, we need to first code a decision tree that supports weighting of individual data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twJD9n8joM3b"
   },
   "source": [
    "### Weighted error definition\n",
    "\n",
    "Consider a model with $N$ data points with:\n",
    "* Predictions $\\hat{y}_1 ... \\hat{y}_n$ \n",
    "* Target $y_1 ... y_n$ \n",
    "* Data point weights $\\alpha_1 ... \\alpha_n$.\n",
    "\n",
    "Then the **weighted error** is defined by:\n",
    "$$\n",
    "\\mathrm{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}}) = \\frac{\\sum_{i=1}^{n} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}]}{\\sum_{i=1}^{n} \\alpha_i}\n",
    "$$\n",
    "where $1[y_i \\neq \\hat{y_i}]$ is an indicator function that is set to $1$ if $y_i \\neq \\hat{y_i}$.\n",
    "\n",
    "\n",
    "### Write a function to compute weight of mistakes\n",
    "\n",
    "Write a function that calculates the weight of mistakes for making the \"weighted-majority\" predictions for a dataset. The function accepts two inputs:\n",
    "* `labels_in_node`: Targets $y_1 ... y_n$ \n",
    "* `data_weights`: Data point weights $\\alpha_1 ... \\alpha_n$\n",
    "\n",
    "We are interested in computing the (total) weight of mistakes, i.e.\n",
    "$$\n",
    "\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}}) = \\sum_{i=1}^{n} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}].\n",
    "$$\n",
    "This quantity is analogous to the number of mistakes, except that each mistake now carries different weight. It is related to the weighted error in the following way:\n",
    "$$\n",
    "\\mathrm{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}}) = \\frac{\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})}{\\sum_{i=1}^{n} \\alpha_i}\n",
    "$$\n",
    "\n",
    "The function **intermediate_node_weighted_mistakes** should first compute two weights: \n",
    " * $\\mathrm{WM}_{-1}$: weight of mistakes when all predictions are $\\hat{y}_i = -1$ i.e $\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{-1}$)\n",
    " * $\\mathrm{WM}_{+1}$: weight of mistakes when all predictions are $\\hat{y}_i = +1$ i.e $\\mbox{WM}(\\mathbf{\\alpha}, \\mathbf{+1}$)\n",
    " \n",
    " where $\\mathbf{-1}$ and $\\mathbf{+1}$ are vectors where all values are -1 and +1 respectively.\n",
    " \n",
    "After computing $\\mathrm{WM}_{-1}$ and $\\mathrm{WM}_{+1}$, the function **intermediate_node_weighted_mistakes** should return the lower of the two weights of mistakes, along with the class associated with that weight. We have provided a skeleton for you with `YOUR CODE HERE` to be filled in several places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "2Ji84PVdoM3c"
   },
   "outputs": [],
   "source": [
    "def intermediate_node_weighted_mistakes(labels_in_node, data_weights):\n",
    "    # Sum the weights of all entries with label +1\n",
    "    total_weight_positive = sum(data_weights[labels_in_node == +1])\n",
    "    \n",
    "    # Weight of mistakes for predicting all -1's is equal to the sum above\n",
    "    ### YOUR CODE HERE\n",
    "    weight_mistake_all_negative = total_weight_positive\n",
    "    \n",
    "    # Sum the weights of all entries with label -1\n",
    "    ### YOUR CODE HERE\n",
    "    total_weight_negative = sum(data_weights[labels_in_node == -1])\n",
    "    \n",
    "    # Weight of mistakes for predicting all +1's is equal to the sum above\n",
    "    ### YOUR CODE HERE\n",
    "    weight_mistake_all_positive = total_weight_negative\n",
    "    \n",
    "    # Return the tuple (weight, class_label) representing the lower of the two weights\n",
    "    #    class_label should be an integer of value +1 or -1.\n",
    "    # If the two weights are identical, return (weighted_mistakes_all_positive,+1)\n",
    "    if weight_mistake_all_negative < weight_mistake_all_positive:\n",
    "        return (weight_mistake_all_negative, -1)\n",
    "    else:\n",
    "        return(weight_mistake_all_positive,+1)\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKV3AZF7oM3c"
   },
   "source": [
    "**Checkpoint:** Test your **intermediate_node_weighted_mistakes** function, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "RbHgEMrLoM3c",
    "outputId": "b02f59cb-7030-4d04-c5b7-546b2b74883e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "example_labels = pd.Series([-1, -1, 1, 1, 1])\n",
    "example_data_weights = pd.Series([1., 2., .5, 1., 1.])\n",
    "if intermediate_node_weighted_mistakes(example_labels, example_data_weights) == (2.5, -1):\n",
    "    print('Test passed!')\n",
    "else:\n",
    "    print('Test failed... try again!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17FnXUJToM3d"
   },
   "source": [
    "Recall that the **classification error** is defined as follows:\n",
    "$$\n",
    "\\mbox{classification error} = \\frac{\\mbox{# mistakes}}{\\mbox{# all data points}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGxvIGqPoM3d"
   },
   "source": [
    "### Function to pick best feature to split on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJBxqVVPoM3d"
   },
   "source": [
    "The next step is to pick the best feature to split on.\n",
    "\n",
    "The **best_splitting_feature** function takes the data, the festures, the targetm and the data weights as input and returns the best feature to split on.\n",
    "  \n",
    "Complete the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "s9aSbft0oM3d"
   },
   "outputs": [],
   "source": [
    "# If the data is identical in each feature, this function should return None\n",
    "\n",
    "def best_splitting_feature(data, features, target, data_weights):\n",
    "    \n",
    "    # These variables will keep track of the best feature and the corresponding error\n",
    "    best_feature = None\n",
    "    best_error = float('+inf') \n",
    "    num_points = float(len(data))\n",
    "\n",
    "    # Loop through each feature to consider splitting on that feature\n",
    "    for feature in features:\n",
    "        \n",
    "        # The left split will have all data points where the feature value is 0\n",
    "        # The right split will have all data points where the feature value is 1\n",
    "        left_split = data[data[feature] == 0]\n",
    "        right_split = data[data[feature] == 1]\n",
    "        \n",
    "        # Apply the same filtering to data_weights to create left_data_weights, right_data_weights\n",
    "        ## YOUR CODE HERE\n",
    "        left_data_weights = data_weights[data[feature] == 0]\n",
    "        right_data_weights = data_weights[data[feature] ==1]\n",
    "                    \n",
    "        # Calculate the weight of mistakes for left and right sides\n",
    "        ## YOUR CODE HERE\n",
    "        left_weight_mistake = intermediate_node_weighted_mistakes(left_split[target], left_data_weights)\n",
    "        right_weight_mistake = intermediate_node_weighted_mistakes(right_split[target], right_data_weights)\n",
    "        \n",
    "        # Compute weighted error by computing\n",
    "        #  ( [weight of mistakes (left)] + [weight of mistakes (right)] ) / [total weight of all data points]\n",
    "        ## YOUR CODE HERE\n",
    "        error = (left_weight_mistake[0] + right_weight_mistake[0])/sum(data_weights)\n",
    "        \n",
    "        # If this is the best error we have found so far, store the feature and the error\n",
    "        if error < best_error:\n",
    "            best_feature = feature\n",
    "            best_error = error\n",
    "    \n",
    "    # Return the best feature we found\n",
    "    return best_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4FTq7LaoM3d"
   },
   "source": [
    "**Checkpoint:** Now, we have another checkpoint to make sure you are on the right track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "GWdkKTrwoM3d",
    "outputId": "ec370f31-453b-4e2b-b401-ed368954c990"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "example_data_weights = np.array(len(train_data)* [1.5])\n",
    "if best_splitting_feature(train_data, features, target, example_data_weights) == 'term_ 36 months':\n",
    "    print('Test passed!')\n",
    "else:\n",
    "    print('Test failed... try again!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMOXXv0xoM3e"
   },
   "source": [
    "**Aside**. Relationship between weighted error and weight of mistakes:\n",
    "\n",
    "By definition, the weighted error is the weight of mistakes divided by the weight of all data points, so\n",
    "$$\n",
    "\\mathrm{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}}) = \\frac{\\sum_{i=1}^{n} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}]}{\\sum_{i=1}^{n} \\alpha_i} = \\frac{\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})}{\\sum_{i=1}^{n} \\alpha_i}.\n",
    "$$\n",
    "\n",
    "In the code above, we obtain $\\mathrm{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})$ from the two weights of mistakes from both sides, $\\mathrm{WM}(\\mathbf{\\alpha}_{\\mathrm{left}}, \\mathbf{\\hat{y}}_{\\mathrm{left}})$ and $\\mathrm{WM}(\\mathbf{\\alpha}_{\\mathrm{right}}, \\mathbf{\\hat{y}}_{\\mathrm{right}})$. First, notice that the overall weight of mistakes $\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})$ can be broken into two weights of mistakes over either side of the split:\n",
    "$$\n",
    "\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})\n",
    "= \\sum_{i=1}^{n} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}]\n",
    "= \\sum_{\\mathrm{left}} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}] + \\sum_{\\mathrm{right}} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}]\\\\\n",
    "= \\mathrm{WM}(\\mathbf{\\alpha}_{\\mathrm{left}}, \\mathbf{\\hat{y}}_{\\mathrm{left}}) + \\mathrm{WM}(\\mathbf{\\alpha}_{\\mathrm{right}}, \\mathbf{\\hat{y}}_{\\mathrm{right}})\n",
    "$$\n",
    "We then divide through by the total weight of all data points to obtain $\\mathrm{E}({\\alpha}, \\mathbf{\\hat{y}})$:\n",
    "$$\n",
    "\\mathrm{E}({\\alpha}, \\mathbf{\\hat{y}})\n",
    "= \\frac{\\mathrm{WM}({\\alpha}_{\\mathrm{left}}, \\mathbf{\\hat{y}}_{\\mathrm{left}}) + \\mathrm{WM}({\\alpha}_{\\mathrm{right}}, \\mathbf{\\hat{y}}_{\\mathrm{right}})}{\\sum_{i=1}^{n} \\alpha_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0xaw0l5OoM3e"
   },
   "source": [
    "### Building the tree\n",
    "\n",
    "With the above functions implemented correctly, we are now ready to build our decision tree. A decision tree will be represented as a dictionary which contains the following keys:\n",
    "\n",
    "    { \n",
    "       'is_leaf'            : True/False.\n",
    "       'prediction'         : Prediction at the leaf node.\n",
    "       'left'               : (dictionary corresponding to the left tree).\n",
    "       'right'              : (dictionary corresponding to the right tree).\n",
    "       'features_remaining' : List of features that are posible splits.\n",
    "    }\n",
    "    \n",
    "Let us start with a function that creates a leaf node given a set of target values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "aSYAlyifoM3e"
   },
   "outputs": [],
   "source": [
    "def create_leaf(target_values, data_weights):\n",
    "    \n",
    "    # Create a leaf node\n",
    "    leaf = {'splitting_feature' : None,\n",
    "            'is_leaf': True}\n",
    "    \n",
    "    # Computed weight of mistakes.\n",
    "    weighted_error, best_class = intermediate_node_weighted_mistakes(target_values, data_weights)\n",
    "    # Store the predicted class (1 or -1) in leaf['prediction']\n",
    "    ## YOUR CODE HERE\n",
    "    leaf['prediction'] = best_class\n",
    "    \n",
    "    return leaf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qOo-z0qoM3e"
   },
   "source": [
    "We provide a function that learns a weighted decision tree recursively and implements 3 stopping conditions:\n",
    "1. All data points in a node are from the same class.\n",
    "2. No more features to split on.\n",
    "3. Stop growing the tree when the tree depth reaches **max_depth**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "hwk8iJM2oM3e"
   },
   "outputs": [],
   "source": [
    "def weighted_decision_tree_create(data, features, target, data_weights, current_depth = 1, max_depth = 10):\n",
    "    remaining_features = features[:] # Make a copy of the features.\n",
    "    target_values = data[target]\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    print(\"Subtree, depth = %s (%s data points).\" % (current_depth, len(target_values)))\n",
    "    # Stopping condition 1. Error is 0.\n",
    "    if intermediate_node_weighted_mistakes(target_values, data_weights)[0] <= 1e-15:\n",
    "        print(\"Stopping condition 1 reached.\")                \n",
    "        return create_leaf(target_values, data_weights)\n",
    "    \n",
    "    # Stopping condition 2. No more features.\n",
    "    if remaining_features == []:\n",
    "        print(\"Stopping condition 2 reached.\")                \n",
    "        return create_leaf(target_values, data_weights)    \n",
    "    \n",
    "    # Additional stopping condition (limit tree depth)\n",
    "    if current_depth > max_depth:\n",
    "        print(\"Reached maximum depth. Stopping for now.\")\n",
    "        return create_leaf(target_values, data_weights)\n",
    "    \n",
    "    # If all the datapoints are the same, splitting_feature will be None. Create a leaf\n",
    "    splitting_feature = best_splitting_feature(data, features, target, data_weights)\n",
    "    remaining_features.remove(splitting_feature)\n",
    "        \n",
    "    left_split = data[data[splitting_feature] == 0]\n",
    "    right_split = data[data[splitting_feature] == 1]\n",
    "    \n",
    "    left_data_weights = data_weights[data[splitting_feature] == 0]\n",
    "    right_data_weights = data_weights[data[splitting_feature] == 1]\n",
    "    \n",
    "    print(\"Split on feature %s. (%s, %s)\" % (\\\n",
    "              splitting_feature, len(left_split), len(right_split)))\n",
    "    \n",
    "    # Create a leaf node if the split is \"perfect\"\n",
    "    if len(left_split) == len(data):\n",
    "        print(\"Creating leaf node.\")\n",
    "        return create_leaf(left_split[target], data_weights)\n",
    "    if len(right_split) == len(data):\n",
    "        print(\"Creating leaf node.\")\n",
    "        return create_leaf(right_split[target], data_weights)\n",
    "    \n",
    "    # Repeat (recurse) on left and right subtrees\n",
    "    ## YOUR CODE HERE\n",
    "    current_depth+=1 \n",
    "    left_tree = weighted_decision_tree_create(left_split,remaining_features, \n",
    "                                              target, left_data_weights, current_depth, max_depth)\n",
    "    right_tree = weighted_decision_tree_create(right_split,remaining_features, \n",
    "                                              target, right_data_weights, current_depth, max_depth)\n",
    "    return {'is_leaf'          : False, \n",
    "            'prediction'       : None,\n",
    "            'splitting_feature': splitting_feature,\n",
    "            'left'             : left_tree, \n",
    "            'right'            : right_tree}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w2ut1sv4oM3f"
   },
   "source": [
    "Here is a recursive function to count the nodes in your tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "Nk-0__mcoM3f"
   },
   "outputs": [],
   "source": [
    "def count_nodes(tree):\n",
    "    if tree['is_leaf']:\n",
    "        return 1\n",
    "    return 1 + count_nodes(tree['left']) + count_nodes(tree['right'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WCyY0avgoM3f"
   },
   "source": [
    "Run the following test code to check your implementation. Make sure you get **'Test passed'** before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "ZUkAqpFroM3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxdepth 2\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature term_ 36 months. (8850, 23150)\n",
      "maxdepth 2\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8850 data points).\n",
      "Split on feature grade_A. (8775, 75)\n",
      "maxdepth 2\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (8775 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "maxdepth 2\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (75 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "maxdepth 2\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23150 data points).\n",
      "Split on feature grade_D. (19331, 3819)\n",
      "maxdepth 2\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (19331 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "maxdepth 2\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (3819 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "example_data_weights = np.array([1.0 for i in range(len(train_data))])\n",
    "small_data_decision_tree = weighted_decision_tree_create(train_data, features, target,\n",
    "                                        example_data_weights, max_depth=2)\n",
    "if count_nodes(small_data_decision_tree) == 7:\n",
    "    print('Test passed!')\n",
    "else:\n",
    "    print('Test failed... try again!')\n",
    "    print('Number of nodes found:', count_nodes(small_data_decision_tree))\n",
    "    print('Number of nodes that should be there: 7') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8lddEbCoM3f"
   },
   "source": [
    "Let us take a quick look at what the trained tree is like. You should get something that looks like the following\n",
    "\n",
    "```\n",
    "{'is_leaf': False,\n",
    "    'left': {'is_leaf': False,\n",
    "        'left': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
    "        'prediction': None,\n",
    "        'right': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
    "        'splitting_feature': 'grade_A'\n",
    "     },\n",
    "    'prediction': None,\n",
    "    'right': {'is_leaf': False,\n",
    "        'left': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
    "        'prediction': None,\n",
    "        'right': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
    "        'splitting_feature': 'grade_D'\n",
    "     },\n",
    "     'splitting_feature': 'term. 36 months'\n",
    "}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "KTdds9xeoM3f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_leaf': False,\n",
       " 'prediction': None,\n",
       " 'splitting_feature': 'term_ 36 months',\n",
       " 'left': {'is_leaf': False,\n",
       "  'prediction': None,\n",
       "  'splitting_feature': 'grade_A',\n",
       "  'left': {'splitting_feature': None, 'is_leaf': True, 'prediction': -1},\n",
       "  'right': {'splitting_feature': None, 'is_leaf': True, 'prediction': 1}},\n",
       " 'right': {'is_leaf': False,\n",
       "  'prediction': None,\n",
       "  'splitting_feature': 'grade_D',\n",
       "  'left': {'splitting_feature': None, 'is_leaf': True, 'prediction': 1},\n",
       "  'right': {'splitting_feature': None, 'is_leaf': True, 'prediction': -1}}}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_data_decision_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8E4rHcdMoM3f"
   },
   "source": [
    "### Making predictions with a weighted decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8_Lpq8ioM3g"
   },
   "source": [
    "We give you a function that classifies one data point. It can also return the probability if you want to play around with that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "exYUKHHEoM3g"
   },
   "outputs": [],
   "source": [
    "def classify(tree, x, annotate = False):   \n",
    "    # If the node is a leaf node.\n",
    "    if tree['is_leaf']:\n",
    "        if annotate: \n",
    "            print(\"At leaf, predicting %s\" % tree['prediction'])\n",
    "        return tree['prediction'] \n",
    "    else:\n",
    "        # Split on feature.\n",
    "        split_feature_value = x[tree['splitting_feature']]\n",
    "        if annotate: \n",
    "            print(\"Split on %s = %s\" % (tree['splitting_feature'], split_feature_value))\n",
    "        if split_feature_value == 0:\n",
    "            return classify(tree['left'], x, annotate)\n",
    "        else:\n",
    "            return classify(tree['right'], x, annotate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dRGCRG9CoM3g"
   },
   "source": [
    "### Evaluating the tree\n",
    "\n",
    "Now, we will write a function to evaluate a decision tree by computing the classification error of the tree on the given dataset.\n",
    "\n",
    "Again, recall that the **classification error** is defined as follows:\n",
    "$$\n",
    "\\mbox{classification error} = \\frac{\\mbox{# mistakes}}{\\mbox{# all data points}}\n",
    "$$\n",
    "\n",
    "The function called **evaluate_classification_error** takes in as input:\n",
    "1. `tree` (as described above)\n",
    "2. `data` (a dataframe)\n",
    "\n",
    "The function does not change because of adding data point weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "G9j5bC8KoM3g"
   },
   "outputs": [],
   "source": [
    "def evaluate_classification_error(tree, data):\n",
    "    # Apply the classify(tree, x) to each row in your data\n",
    "    prediction = data.apply(axis = 1, func = lambda x : classify(tree,x)) \n",
    "    \n",
    "    # Once you've made the predictions, calculate the classification error\n",
    "    return (prediction != data[target]).sum() / float(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "ie4oHq5LoM3g"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.390875"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classification_error(small_data_decision_tree, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BnlAxBRWoM3g"
   },
   "source": [
    "### Example: Training a weighted decision tree\n",
    "\n",
    "To build intuition on how weighted data points affect the tree being built, consider the following:\n",
    "\n",
    "Suppose we only care about making good predictions for the **first 10 and last 10 items** in `train_data`, we assign weights:\n",
    "* 1 to the last 10 items \n",
    "* 1 to the first 10 items \n",
    "* and 0 to the rest. \n",
    "\n",
    "Let us fit a weighted decision tree with `max_depth = 2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "Ofh6zAIZoM3g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxdepth 2\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature emp_length_10+ years. (22413, 9587)\n",
      "maxdepth 2\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (22413 data points).\n",
      "Split on feature grade_A. (19673, 2740)\n",
      "maxdepth 2\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (19673 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "maxdepth 2\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (2740 data points).\n",
      "Stopping condition 1 reached.\n",
      "maxdepth 2\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9587 data points).\n",
      "Stopping condition 1 reached.\n"
     ]
    }
   ],
   "source": [
    "# Assign weights\n",
    "example_data_weights = np.array([1.] * 10 + [0.]*(len(train_data) - 20) + [1.] * 10)\n",
    "\n",
    "# Train a weighted decision tree model.\n",
    "small_data_decision_tree_subset_20 = weighted_decision_tree_create(train_data, features, target,\n",
    "                         example_data_weights, max_depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZiT89z6NoM3g"
   },
   "source": [
    "Now, we will compute the classification error on the `subset_20`, i.e. the subset of data points whose weight is 1 (namely the first and last 10 data points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "nj2bMpn_oM3g"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_20 = train_data.head(10).append(train_data.tail(10))\n",
    "evaluate_classification_error(small_data_decision_tree_subset_20, subset_20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxtDJtR4oM3h"
   },
   "source": [
    "Now, let us compare the classification error of the model `small_data_decision_tree_subset_20` on the entire test set `train_data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "sLyTsCG7oM3h"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.445625"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classification_error(small_data_decision_tree_subset_20, train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfcCmF2hoM3h"
   },
   "source": [
    "The model `small_data_decision_tree_subset_20` performs **a lot** better on `subset_20` than on `train_data`.\n",
    "\n",
    "So, what does this mean?\n",
    "* The points with higher weights are the ones that are more important during the training process of the weighted decision tree.\n",
    "* The points with zero weights are basically ignored during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgrPLHjCoM3h"
   },
   "source": [
    "# Implementing your own Adaboost (on decision stumps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OoZTr7_ioM3h"
   },
   "source": [
    "Now that we have a weighted decision tree working, it takes only a bit of work to implement Adaboost. For the sake of simplicity, let us stick with **decision tree stumps** by training trees with **`max_depth=1`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJnQffDooM3h"
   },
   "source": [
    "Recall from the lecture notes the procedure for Adaboost:\n",
    "\n",
    "1\\. Start with unweighted data with $\\alpha_j = 1$\n",
    "\n",
    "2\\. For t = 1,...T:\n",
    "  * Learn $f_t(x)$ with data weights $\\alpha_j$\n",
    "  * Compute coefficient $\\hat{w}_t$:\n",
    "     $$\\hat{w}_t = \\frac{1}{2}\\ln{\\left(\\frac{1- \\mbox{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})}{\\mbox{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})}\\right)}$$\n",
    "  * Re-compute weights $\\alpha_j$:\n",
    "     $$\\alpha_j \\gets \\begin{cases}\n",
    "     \\alpha_j \\exp{(-\\hat{w}_t)} & \\text{ if }f_t(x_j) = y_j\\\\\n",
    "     \\alpha_j \\exp{(\\hat{w}_t)} & \\text{ if }f_t(x_j) \\neq y_j\n",
    "     \\end{cases}$$\n",
    "  * Normalize weights $\\alpha_j$:\n",
    "      $$\\alpha_j \\gets \\frac{\\alpha_j}{\\sum_{i=1}^{N}{\\alpha_i}} $$\n",
    "  \n",
    "Complete the skeleton for the following code to implement **adaboost_with_tree_stumps**. Fill in the places with `YOUR CODE HERE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "EV41d71eoM3h"
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "from math import exp\n",
    "\n",
    "def adaboost_with_tree_stumps(data, features, target, num_tree_stumps):\n",
    "    # start with unweighted data (uniformly weighted)\n",
    "    alpha =  np.array([1.]*len(data))\n",
    "    weights = []\n",
    "    tree_stumps = []\n",
    "    target_values = data[target]\n",
    "    \n",
    "    for t in range(num_tree_stumps):\n",
    "        print('=====================================================')\n",
    "        print('Adaboost Iteration %d' % t)\n",
    "        print('=====================================================')        \n",
    "        # Learn a weighted decision tree stump. Use max_depth=1\n",
    "        # YOUR CODE HERE\n",
    "        tree_stump = weighted_decision_tree_create(data, features, target, alpha, current_depth = 1, max_depth = 1)\n",
    "        tree_stumps.append(tree_stump)\n",
    "        \n",
    "        # Make predictions\n",
    "        ## YOUR CODE HERE\n",
    "        predictions = data.apply(axis = 1, func = lambda x : classify(tree_stump,x)) \n",
    "        \n",
    "        # Produce a Boolean array indicating whether\n",
    "        # each data point was correctly classified\n",
    "        is_correct = predictions == target_values\n",
    "        is_wrong   = predictions != target_values\n",
    "        \n",
    "        # Compute weighted error\n",
    "        ## YOUR CODE HERE\n",
    "        total_mistake = sum(alpha[is_wrong])\n",
    "        weighted_error = total_mistake/sum(alpha)\n",
    "        \n",
    "        # Compute model coefficient using weighted error\n",
    "        ## YOUR CODE HERE\n",
    "        weight = 0.5*log((1-weighted_error)/weighted_error)\n",
    "        weights.append(weight)\n",
    "        # Adjust weights on data point\n",
    "        ## YOUR CODE HERE\n",
    "        adjustment = is_correct.apply(lambda is_correct : exp(-weight) if is_correct else exp(weight))        \n",
    "        # Scale alpha by multiplying by adjustment \n",
    "        # Then normalize data points weights\n",
    "        ## YOUR CODE HERE \n",
    "        alpha = alpha*adjustment\n",
    "        alpha = alpha/sum(alpha)\n",
    "\n",
    "    \n",
    "    return weights, tree_stumps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z9uue4EKoM3h"
   },
   "source": [
    "### Checking your Adaboost code\n",
    "\n",
    "Train an ensemble of **two** tree stumps and see which features those stumps split on. We will run the algorithm with the following parameters:\n",
    "* `train_data`\n",
    "* `features`\n",
    "* `target`\n",
    "* `num_tree_stumps = 2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "l1CtP2FToM3h",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "Adaboost Iteration 0\n",
      "=====================================================\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature term_ 36 months. (8850, 23150)\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8850 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23150 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 1\n",
      "=====================================================\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_A. (28081, 3919)\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28081 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3919 data points).\n",
      "Reached maximum depth. Stopping for now.\n"
     ]
    }
   ],
   "source": [
    "stump_weights, tree_stumps = adaboost_with_tree_stumps(train_data, features, target, num_tree_stumps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "gvst51tqoM3h"
   },
   "outputs": [],
   "source": [
    "def print_stump(tree):\n",
    "    split_name = tree['splitting_feature'] # split_name is something like 'term. 36 months'\n",
    "    if split_name is None:\n",
    "        print(\"(leaf, label: %s)\" % tree['prediction'])\n",
    "        return None\n",
    "    print('                       root')\n",
    "    print('         |---------------|----------------|')\n",
    "    print('         |                                |')\n",
    "    print('         |                                |')\n",
    "    print('         |                                |')\n",
    "    print('  [{0} == 0]{1}[{0} == 1]    '.format(split_name, ' '*(27-len(split_name))))\n",
    "    print('         |                                |')\n",
    "    print('         |                                |')\n",
    "    print('         |                                |')\n",
    "    print('    (%s)                 (%s)' \\\n",
    "        % (('leaf, label: ' + str(tree['left']['prediction']) if tree['left']['is_leaf'] else 'subtree'),\n",
    "           ('leaf, label: ' + str(tree['right']['prediction']) if tree['right']['is_leaf'] else 'subtree')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvtTHBB8oM3i"
   },
   "source": [
    "Here is what the first stump looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "5TYrf2xfoM3i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       root\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [term_ 36 months == 0]            [term_ 36 months == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (leaf, label: -1)                 (leaf, label: 1)\n"
     ]
    }
   ],
   "source": [
    "print_stump(tree_stumps[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opH5eDa4oM3i"
   },
   "source": [
    "Here is what the next stump looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "tQbcR1n7oM3i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       root\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [grade_A == 0]                    [grade_A == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (leaf, label: -1)                 (leaf, label: 1)\n"
     ]
    }
   ],
   "source": [
    "print_stump(tree_stumps[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "w3g-o3lzoM3i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.17198848113764034, 0.1772878063726963]\n"
     ]
    }
   ],
   "source": [
    "print(stump_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDw70ahCoM3i"
   },
   "source": [
    "If your Adaboost is correctly implemented, the following things should be true:\n",
    "\n",
    "* `tree_stumps[0]` should split on **term. 36 months** with the prediction -1 on the left and +1 on the right.\n",
    "* `tree_stumps[1]` should split on **grade.A** with the prediction -1 on the left and +1 on the right.\n",
    "* Weights should be approximately `[0.17, 0.18]` \n",
    "\n",
    "**Reminders**\n",
    "- Stump weights ($\\mathbf{\\hat{w}}$) and data point weights ($\\mathbf{\\alpha}$) are two different concepts.\n",
    "- Stump weights ($\\mathbf{\\hat{w}}$) tell you how important each stump is while making predictions with the entire boosted ensemble.\n",
    "- Data point weights ($\\mathbf{\\alpha}$) tell you how important each data point is while training a decision stump."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-34Oj_doM3i"
   },
   "source": [
    "### Training a boosted ensemble of 10 stumps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oUMM7QWoM3i"
   },
   "source": [
    "Let us train an ensemble of 10 decision tree stumps with Adaboost. We run the **adaboost_with_tree_stumps** function with the following parameters:\n",
    "* `train_data`\n",
    "* `features`\n",
    "* `target`\n",
    "* `num_tree_stumps = 10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "rKrIwzS8oM3i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "Adaboost Iteration 0\n",
      "=====================================================\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature term_ 36 months. (8850, 23150)\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8850 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23150 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 1\n",
      "=====================================================\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_A. (28081, 3919)\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28081 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3919 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 2\n",
      "=====================================================\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_D. (26027, 5973)\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (26027 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5973 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 3\n",
      "=====================================================\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_B. (23457, 8543)\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23457 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8543 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 4\n",
      "=====================================================\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_E. (28766, 3234)\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28766 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3234 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 5\n",
      "=====================================================\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (16870, 15130)\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (16870 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (15130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 6\n",
      "=====================================================\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_A. (28081, 3919)\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28081 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3919 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 7\n",
      "=====================================================\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_F. (30624, 1376)\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30624 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1376 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 8\n",
      "=====================================================\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_A. (28081, 3919)\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28081 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3919 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 9\n",
      "=====================================================\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_E. (28766, 3234)\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28766 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3234 data points).\n",
      "Reached maximum depth. Stopping for now.\n"
     ]
    }
   ],
   "source": [
    "stump_weights, tree_stumps = adaboost_with_tree_stumps(train_data, features, \n",
    "                                target, num_tree_stumps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FkMv_jt1dO8y"
   },
   "source": [
    "### Plot the boosted stumps in the additive model\n",
    "\n",
    "The decision stumps picks a feature and a threshold, visualize them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "MeNYytx5dkMa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 1th stump is \n",
      "                       root\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [term_ 36 months == 0]            [term_ 36 months == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (leaf, label: -1)                 (leaf, label: 1)\n",
      "The 2th stump is \n",
      "                       root\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [grade_A == 0]                    [grade_A == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (leaf, label: -1)                 (leaf, label: 1)\n",
      "The 3th stump is \n",
      "                       root\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [grade_D == 0]                    [grade_D == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (leaf, label: 1)                 (leaf, label: -1)\n",
      "The 4th stump is \n",
      "                       root\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [grade_B == 0]                    [grade_B == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (leaf, label: -1)                 (leaf, label: 1)\n",
      "The 5th stump is \n",
      "                       root\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [grade_E == 0]                    [grade_E == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (leaf, label: 1)                 (leaf, label: -1)\n",
      "The 6th stump is \n",
      "                       root\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [home_ownership_MORTGAGE == 0]    [home_ownership_MORTGAGE == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (leaf, label: -1)                 (leaf, label: 1)\n",
      "The 7th stump is \n",
      "                       root\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [grade_A == 0]                    [grade_A == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (leaf, label: -1)                 (leaf, label: 1)\n",
      "The 8th stump is \n",
      "                       root\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [grade_F == 0]                    [grade_F == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (leaf, label: 1)                 (leaf, label: -1)\n",
      "The 9th stump is \n",
      "                       root\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [grade_A == 0]                    [grade_A == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (leaf, label: -1)                 (leaf, label: 1)\n",
      "The 10th stump is \n",
      "                       root\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [grade_E == 0]                    [grade_E == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (leaf, label: 1)                 (leaf, label: -1)\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "i = 0 \n",
    "for tree in tree_stumps:\n",
    "    i+=1\n",
    "    print(f\"The {i}th stump is \")\n",
    "    print_stump(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bkNoD_UvoM3j"
   },
   "source": [
    "## Making predictions\n",
    "\n",
    "Recall from the lecture that in order to make predictions, we use the following formula:\n",
    "$$\n",
    "\\hat{y} = sign\\left(\\sum_{t=1}^T \\hat{w}_t f_t(x)\\right)\n",
    "$$\n",
    "\n",
    "We need to do the following things:\n",
    "- Compute the predictions $f_t(x)$ using the $t$-th decision tree\n",
    "- Compute $\\hat{w}_t f_t(x)$ by multiplying the `stump_weights` with the predictions $f_t(x)$ from the decision trees\n",
    "- Sum the weighted predictions over each stump in the ensemble.\n",
    "\n",
    "Complete the following skeleton for making predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "M9S4UTFIoM3j"
   },
   "outputs": [],
   "source": [
    "def predict_adaboost(stump_weights, tree_stumps, data):\n",
    "    scores = np.array([0.]*len(data))\n",
    "    \n",
    "    for i, tree_stump in enumerate(tree_stumps):\n",
    "        predictions = data.apply(lambda x: classify(tree_stump, x), axis = 1)\n",
    "        \n",
    "        # Accumulate predictions on scores array\n",
    "        # YOUR CODE HERE\n",
    "        scores+= predictions*stump_weights[i]\n",
    "        \n",
    "    return scores.apply(lambda score : +1 if score > 0 else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "yltvmKauoM3j"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 10-component ensemble = 0.62825\n"
     ]
    }
   ],
   "source": [
    "predictions = predict_adaboost(stump_weights, tree_stumps, test_data)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(test_data[target], predictions)\n",
    "print('Accuracy of 10-component ensemble = %s' % accuracy) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SU7snIBqoM3j"
   },
   "source": [
    "Now, let us take a quick look what the `stump_weights` look like at the end of each iteration of the 10-stump ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "oiz9BkA0oM3j"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.17198848113764034,\n",
       " 0.1772878063726963,\n",
       " 0.10308067697010909,\n",
       " 0.08686702058336851,\n",
       " 0.07220085937793974,\n",
       " 0.07438562925258671,\n",
       " 0.05834552873244469,\n",
       " 0.04545487026475097,\n",
       " 0.0319454846001187,\n",
       " 0.023305292432239024]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stump_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vbxmBB78oM3j"
   },
   "source": [
    "**Question** i: Are the weights monotonically decreasing, monotonically increasing, or neither?\n",
    "\n",
    "The weights are roughly monotonically decreasing \n",
    "\n",
    "**Reminder**: Stump weights ($\\mathbf{\\hat{w}}$) tell you how important each stump is while making predictions with the entire boosted ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OffpbikcoM3j"
   },
   "source": [
    "# Performance plots\n",
    "\n",
    "In this section, we will try to reproduce some performance plots.\n",
    "\n",
    "### How does accuracy change with adding stumps to the ensemble?\n",
    "\n",
    "We will now train an ensemble with:\n",
    "* `train_data`\n",
    "* `features`\n",
    "* `target`\n",
    "* `num_tree_stumps = 30`\n",
    "\n",
    "Once we are done with this, we will then do the following:\n",
    "* Compute the classification error at the end of each iteration.\n",
    "* Plot a curve of classification error vs iteration.\n",
    "\n",
    "First, lets train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "DfpOHylUoM3j",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "Adaboost Iteration 0\n",
      "=====================================================\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature term_ 36 months. (8850, 23150)\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8850 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23150 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 1\n",
      "=====================================================\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_A. (28081, 3919)\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28081 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3919 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 2\n",
      "=====================================================\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_D. (26027, 5973)\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (26027 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5973 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 3\n",
      "=====================================================\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_B. (23457, 8543)\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23457 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8543 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 4\n",
      "=====================================================\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_E. (28766, 3234)\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28766 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3234 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 5\n",
      "=====================================================\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (16870, 15130)\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (16870 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (15130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 6\n",
      "=====================================================\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_A. (28081, 3919)\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28081 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3919 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 7\n",
      "=====================================================\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_F. (30624, 1376)\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30624 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1376 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 8\n",
      "=====================================================\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_A. (28081, 3919)\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28081 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3919 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 9\n",
      "=====================================================\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_E. (28766, 3234)\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28766 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3234 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 10\n",
      "=====================================================\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature term_ 36 months. (8850, 23150)\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8850 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23150 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 11\n",
      "=====================================================\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_F. (30624, 1376)\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30624 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1376 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 12\n",
      "=====================================================\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature emp_length_10+ years. (22413, 9587)\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (22413 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9587 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 13\n",
      "=====================================================\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split on feature grade_B. (23457, 8543)\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23457 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8543 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 14\n",
      "=====================================================\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_F. (30624, 1376)\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30624 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1376 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 15\n",
      "=====================================================\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_D. (26027, 5973)\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (26027 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5973 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 16\n",
      "=====================================================\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_F. (30624, 1376)\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30624 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1376 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 17\n",
      "=====================================================\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_A. (28081, 3919)\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28081 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3919 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 18\n",
      "=====================================================\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_E. (28766, 3234)\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28766 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3234 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 19\n",
      "=====================================================\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_C. (23388, 8612)\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23388 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8612 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 20\n",
      "=====================================================\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (16870, 15130)\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (16870 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (15130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 21\n",
      "=====================================================\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature term_ 36 months. (8850, 23150)\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8850 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23150 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 22\n",
      "=====================================================\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_F. (30624, 1376)\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30624 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1376 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 23\n",
      "=====================================================\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_B. (23457, 8543)\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23457 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8543 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 24\n",
      "=====================================================\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature emp_length_2 years. (29104, 2896)\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (29104 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (2896 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 25\n",
      "=====================================================\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_G. (31657, 343)\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (31657 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (343 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 26\n",
      "=====================================================\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_A. (28081, 3919)\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28081 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3919 data points).\n",
      "Reached maximum depth. Stopping for now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "Adaboost Iteration 27\n",
      "=====================================================\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_G. (31657, 343)\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (31657 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (343 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 28\n",
      "=====================================================\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature home_ownership_OWN. (29204, 2796)\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (29204 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (2796 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 29\n",
      "=====================================================\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_G. (31657, 343)\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (31657 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "maxdepth 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (343 data points).\n",
      "Reached maximum depth. Stopping for now.\n"
     ]
    }
   ],
   "source": [
    "# this may take a while... \n",
    "stump_weights, tree_stumps = adaboost_with_tree_stumps(train_data, \n",
    "                                 features, target, num_tree_stumps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9aYX1Bf5oM3j"
   },
   "source": [
    "### Computing training error at the end of each iteration\n",
    "\n",
    "Now, we will compute the classification error on the **train_data** and see how it is reduced as trees are added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "myoLZ4L9oM3j"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, training error = 0.41484374999999996\n",
      "Iteration 2, training error = 0.43281250000000004\n",
      "Iteration 3, training error = 0.39059374999999996\n",
      "Iteration 4, training error = 0.39059374999999996\n",
      "Iteration 5, training error = 0.37931250000000005\n",
      "Iteration 6, training error = 0.38228125\n",
      "Iteration 7, training error = 0.37253125\n",
      "Iteration 8, training error = 0.37549999999999994\n",
      "Iteration 9, training error = 0.37253125\n",
      "Iteration 10, training error = 0.37253125\n",
      "Iteration 11, training error = 0.37253125\n",
      "Iteration 12, training error = 0.37150000000000005\n",
      "Iteration 13, training error = 0.37253125\n",
      "Iteration 14, training error = 0.37150000000000005\n",
      "Iteration 15, training error = 0.37150000000000005\n",
      "Iteration 16, training error = 0.37150000000000005\n",
      "Iteration 17, training error = 0.37150000000000005\n",
      "Iteration 18, training error = 0.37146875\n",
      "Iteration 19, training error = 0.37150000000000005\n",
      "Iteration 20, training error = 0.37146875\n",
      "Iteration 21, training error = 0.37209375\n",
      "Iteration 22, training error = 0.37146875\n",
      "Iteration 23, training error = 0.37212500000000004\n",
      "Iteration 24, training error = 0.37150000000000005\n",
      "Iteration 25, training error = 0.37150000000000005\n",
      "Iteration 26, training error = 0.37212500000000004\n",
      "Iteration 27, training error = 0.37150000000000005\n",
      "Iteration 28, training error = 0.37131250000000005\n",
      "Iteration 29, training error = 0.37121875000000004\n",
      "Iteration 30, training error = 0.37124999999999997\n"
     ]
    }
   ],
   "source": [
    "error_all = []\n",
    "for n in range(1, 31):\n",
    "    predictions = predict_adaboost(stump_weights[:n], tree_stumps[:n], train_data)\n",
    "    error = 1.0 - accuracy_score(train_data[target], predictions)\n",
    "    error_all.append(error)\n",
    "    print(\"Iteration %s, training error = %s\" % (n, error_all[n-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wxw0ZdlHoM3j"
   },
   "source": [
    "### Visualizing training error vs number of iterations\n",
    "\n",
    "We have provided you with a simple code snippet that plots classification error with the number of iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "9Te1Mms-oM3k"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = 7, 5\n",
    "plt.plot(list(range(1,31)), error_all, '-', linewidth=4.0, label='Training error')\n",
    "plt.title('Performance of Adaboost ensemble')\n",
    "plt.xlabel('# of iterations')\n",
    "plt.ylabel('Classification error')\n",
    "plt.legend(loc='best', prop={'size':15})\n",
    "\n",
    "plt.rcParams.update({'font.size': 16})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1mZai_6oM3k"
   },
   "source": [
    "### Evaluation on the test data\n",
    "\n",
    "Performing well on the training data is cheating, so lets make sure it works on the `test_data` as well. Here, we will compute the classification error on the `test_data` at the end of each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "K8Uo8rKuoM3k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, test error = 0.41037500000000005\n",
      "Iteration 2, test error = 0.43174999999999997\n",
      "Iteration 3, test error = 0.390875\n",
      "Iteration 4, test error = 0.390875\n",
      "Iteration 5, test error = 0.37825\n",
      "Iteration 6, test error = 0.382625\n",
      "Iteration 7, test error = 0.37175\n",
      "Iteration 8, test error = 0.37612500000000004\n",
      "Iteration 9, test error = 0.37175\n",
      "Iteration 10, test error = 0.37175\n",
      "Iteration 11, test error = 0.37175\n",
      "Iteration 12, test error = 0.369375\n",
      "Iteration 13, test error = 0.369375\n",
      "Iteration 14, test error = 0.369375\n",
      "Iteration 15, test error = 0.369375\n",
      "Iteration 16, test error = 0.369375\n",
      "Iteration 17, test error = 0.369375\n",
      "Iteration 18, test error = 0.371\n",
      "Iteration 19, test error = 0.369375\n",
      "Iteration 20, test error = 0.371\n",
      "Iteration 21, test error = 0.36924999999999997\n",
      "Iteration 22, test error = 0.371\n",
      "Iteration 23, test error = 0.367625\n",
      "Iteration 24, test error = 0.369375\n",
      "Iteration 25, test error = 0.369375\n",
      "Iteration 26, test error = 0.367625\n",
      "Iteration 27, test error = 0.369375\n",
      "Iteration 28, test error = 0.36950000000000005\n",
      "Iteration 29, test error = 0.369\n",
      "Iteration 30, test error = 0.369\n"
     ]
    }
   ],
   "source": [
    "test_error_all = []\n",
    "for n in range(1, 31):\n",
    "    predictions = predict_adaboost(stump_weights[:n], tree_stumps[:n], test_data)\n",
    "    error = 1.0 - accuracy_score(test_data[target], predictions)\n",
    "    test_error_all.append(error)\n",
    "    print(\"Iteration %s, test error = %s\" % (n, test_error_all[n-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "De4zPXJzoM3k"
   },
   "source": [
    "### Visualize both the training and test errors\n",
    "\n",
    "Now, let us plot the training & test error with the number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "LkxNge6HoM3k"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = 7, 5\n",
    "plt.plot(list(range(1,31)), error_all, '-', linewidth=4.0, label='Training error')\n",
    "plt.plot(list(range(1,31)), test_error_all, '-', linewidth=4.0, label='Test error')\n",
    "\n",
    "plt.title('Performance of Adaboost ensemble')\n",
    "plt.xlabel('# of iterations')\n",
    "plt.ylabel('Classification error')\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.legend(loc='best', prop={'size':15})\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lafYFpn0oM3k"
   },
   "source": [
    "**Question** ii: From this plot (with 30 trees), is there massive overfitting as the # of iterations increases?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4AeYzVfoM3k"
   },
   "source": [
    "I do not think there is a massive overfitting since both the training and testing error are decreasing. Overfitting happens when only the training error decreases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "adaboost.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
